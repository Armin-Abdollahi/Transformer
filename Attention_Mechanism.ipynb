{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7paXxYe54HgS9CTyAZuu0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armin-Abdollahi/Transformer/blob/main/Attention_Mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The **Soft Attention mechanism** is a type of attention that allows the model to focus on different parts of the input for each step of the output."
      ],
      "metadata": {
        "id": "ohq73dMhuNsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SoftAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(SoftAttention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attention = nn.Linear(self.input_dim + self.hidden_dim, 1)\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden):\n",
        "        # Concatenate the hidden state with each encoder output\n",
        "        hidden = hidden.repeat(encoder_outputs.size(0), 1, 1).transpose(0, 1)\n",
        "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [batch_size, seq_len, features]\n",
        "        merged = torch.cat((hidden, encoder_outputs), 2)\n",
        "\n",
        "        # Apply the attention layer to get the attention scores\n",
        "        attention_scores = self.attention(merged).squeeze(2)\n",
        "        attention_scores = F.softmax(attention_scores, dim=1)\n",
        "\n",
        "        # Multiply the scores by the encoder outputs to get the weighted sum\n",
        "        context_vector = torch.bmm(attention_scores.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "        return context_vector, attention_scores\n",
        "\n",
        "# Example usage:\n",
        "# Define the dimensions\n",
        "input_dim = 128  # Size of the encoder output feature vector\n",
        "hidden_dim = 256  # Size of the decoder hidden state\n",
        "\n",
        "# Create the SoftAttention layer\n",
        "attention_layer = SoftAttention(input_dim, hidden_dim)\n",
        "\n",
        "# Assume some random encoder outputs and hidden state\n",
        "encoder_outputs = torch.randn(10, 32, input_dim)  # [seq_len, batch_size, features]\n",
        "hidden = torch.randn(32, hidden_dim)  # [batch_size, hidden_dim]\n",
        "\n",
        "# Get the context vector and attention scores\n",
        "context_vector, attention_scores = attention_layer(encoder_outputs, hidden)"
      ],
      "metadata": {
        "id": "6pBWxnFduNX9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hard attention mechanisms** selectively focus on certain parts of the input data while ignoring the rest, which is different from soft attention that considers all parts with varying weights."
      ],
      "metadata": {
        "id": "922s9lx-u4RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def hard_attention(query, keys, values):\n",
        "    \"\"\"\n",
        "    Implements a simple hard attention mechanism.\n",
        "\n",
        "    Parameters:\n",
        "    query (ndarray): The query vector.\n",
        "    keys (ndarray): The key vectors.\n",
        "    values (ndarray): The value vectors.\n",
        "\n",
        "    Returns:\n",
        "    ndarray: The context vector after applying hard attention.\n",
        "    \"\"\"\n",
        "    # Calculate the dot product between the query and the keys\n",
        "    attention_scores = np.dot(query, keys.T)\n",
        "\n",
        "    # Find the index of the maximum score (hard attention)\n",
        "    max_index = np.argmax(attention_scores)\n",
        "\n",
        "    # Select the value vector corresponding to the maximum score\n",
        "    context_vector = values[max_index]\n",
        "\n",
        "    return context_vector\n",
        "\n",
        "# Example usage:\n",
        "query = np.array([1, 0, 0])\n",
        "keys = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "values = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "context = hard_attention(query, keys, values)\n",
        "print(\"Context Vector:\", context)"
      ],
      "metadata": {
        "id": "-Kv6RojtuNUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hb-PIk4QuNRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MTol1rj8uNO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DAZ1EHqPuNMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wD4VROOFuNJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FOc998x9uNG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JZKFgCccuNEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qAHyVx5quNB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mn7HylyEuM_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement a simple **Squeeze-and-Excitation (SE) attention** module"
      ],
      "metadata": {
        "id": "hVhiaEBTpUDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSgfz93jotFw",
        "outputId": "d3a65148-4d1b-4d55-c999-4dba9db25f12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 64, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        # Define the squeeze operation\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        # Define the excitation operations\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        # Squeeze operation\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        # Excitation operation\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        # Scale the input with the attention weights\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "# Example usage\n",
        "# Define the input tensor with batch size 2, 64 channels, and 32x32 spatial dimensions\n",
        "input_tensor = torch.randn(2, 64, 32, 32)\n",
        "# Create the SE attention layer for 64 channels\n",
        "se_layer = SELayer(channel=64)\n",
        "# Forward pass to obtain the output with attention applied\n",
        "output_tensor = se_layer(input_tensor)\n",
        "\n",
        "print(output_tensor.shape)  # Expected shape: [2, 64, 32, 32]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "npKgtfVw7QDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l2GuIL_F7P_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6QmbNcqO7P9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hDr73p9r7P6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a high-level overview of how you can implement various attention mechanisms in Python, particularly useful for EEG signal processing and other machine learning tasks:"
      ],
      "metadata": {
        "id": "hpfuP1yK74_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Soft Attention\n",
        "class SoftAttention(nn.Module):\n",
        "    def __init__(self, input_dim, attention_dim):\n",
        "        super(SoftAttention, self).__init__()\n",
        "        self.attention_weights = nn.Linear(input_dim, attention_dim)\n",
        "        self.context_vector = nn.Linear(attention_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Compute attention scores\n",
        "        attention_scores = F.tanh(self.attention_weights(inputs))\n",
        "        attention_scores = self.context_vector(attention_scores).squeeze(2)\n",
        "\n",
        "        # Apply softmax to get attention distribution\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "\n",
        "        # Compute weighted sum of inputs\n",
        "        weighted_sum = torch.bmm(attention_weights.unsqueeze(1), inputs).squeeze(1)\n",
        "        return weighted_sum, attention_weights\n",
        "\n",
        "# Hard Attention (Stochastic and not differentiable, usually implemented using Reinforcement Learning)\n",
        "\n",
        "# Self-Attention (Also known as Intra-Attention)\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, input_dim, attention_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = nn.Linear(input_dim, attention_dim)\n",
        "        self.key = nn.Linear(input_dim, attention_dim)\n",
        "        self.value = nn.Linear(input_dim, attention_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        Q = self.query(inputs)\n",
        "        K = self.key(inputs)\n",
        "        V = self.value(inputs)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(K.size(-1), dtype=torch.float32))\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output, attention_weights\n",
        "\n",
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, attention_dim):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_dim = attention_dim\n",
        "        self.heads = nn.ModuleList([SelfAttention(input_dim, attention_dim) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        head_outputs = [head(inputs)[0] for head in self.heads]\n",
        "        concatenated = torch.cat(head_outputs, dim=2)\n",
        "        return concatenated\n",
        "\n",
        "# Cross-Attention, Causal Attention, Global vs. Local Attention\n",
        "# These can be implemented similarly to Self-Attention with modifications to the attention mask or the range of attention.\n",
        "\n"
      ],
      "metadata": {
        "id": "LuhgpvtS7P4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides a starting point for implementing different attention mechanisms. For EEG signal processing, you might need to adjust the input dimensions and possibly incorporate domain-specific knowledge into the attention mechanisms. Remember, the actual implementation details can vary based on the specific requirements of your task and the architecture of your neural network.\n",
        "\n",
        "For Cross-Attention, you would modify the self-attention mechanism to take two different inputs, one serving as queries and the other as keys and values. Causal Attention restricts the attention to only consider previous time steps, which is crucial for tasks like time-series forecasting. Global vs. Local Attention refers to whether the attention mechanism considers all parts of the input sequence (global) or only a subset (local), which can be implemented by modifying the attention mask or using convolutions to restrict the receptive field.\n",
        "\n",
        "Please note that for Hard Attention, due to its stochastic nature, itâ€™s often implemented using reinforcement learning techniques or with approximations that allow for backpropagation, such as the Gumbel-Softmax trick."
      ],
      "metadata": {
        "id": "OKaQiuZr8Cjt"
      }
    }
  ]
}